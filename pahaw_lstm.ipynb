{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import theano\n",
    "#theano.config.openmp=True\n",
    "#set OMP_NUM_THREADS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# CONSTANTS\n",
    "BASE_DIR = os.path.dirname('.') # current dir\n",
    "DB_DIR = os.path.join(BASE_DIR, 'PaHaW', 'PaHaW_public')\n",
    "DB_INFO = os.path.join(BASE_DIR, 'PaHaW', 'PaHaW_files', 'corpus_PaHaW.csv')\n",
    "HEADER = ['y', 'x', 't', 'on_surface', 'azimuth', 'altitude', 'pressure']\n",
    "CHUNK_SIZE=500\n",
    "\n",
    "def load_db(db_dir, db_info, chunk_size=None, header_names=None):    \n",
    "    info_data =pd.read_csv(db_info, delimiter=';')\n",
    "    unique_labels = info_data['Disease']\n",
    "    \n",
    "    subjects_idx = 0\n",
    "    subjects, targets, groups, tasks = [], [], [], []\n",
    "    for path, dirs, files in os.walk(db_dir):\n",
    "        if files:\n",
    "            for file_name in files:\n",
    "                test_file = os.path.join(path, file_name)\n",
    "                task_id = int(file_name.split('_')[2])\n",
    "\n",
    "                # first row contains the number of timesteps\n",
    "                test_data = pd.read_csv(test_file, delimiter=',', skiprows=1, names=header_names)\n",
    "                \n",
    "                # split a sample in several chunks of fixed size\n",
    "                if chunk_size:\n",
    "                    n_splits = int(np.ceil(1.0*len(test_data) / chunk_size))\n",
    "                    chunks = np.array_split(np.array(test_data), n_splits)\n",
    "                else:\n",
    "                    # keep the entire sample\n",
    "                    chunks = [test_data]\n",
    "\n",
    "                # store each element (entire sample or chunks) with the associated\n",
    "                # information (control/parkinson, subject_id, task_id) \n",
    "                for subject_chunk in chunks:                \n",
    "                    subjects.append(subject_chunk)\n",
    "                    targets.append(unique_labels[subjects_idx])\n",
    "                    groups.append(subjects_idx)\n",
    "                    tasks.append(task_id)\n",
    "\n",
    "            subjects_idx += 1\n",
    "    \n",
    "    return subjects, np.array(targets), np.array(groups), np.array(tasks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "# directory containing the experiments\n",
    "#db_dir = DB_DIR\n",
    "db_dir = os.path.join(BASE_DIR, 'PaHaW', 'PaHaW_public_kf')\n",
    "\n",
    "# file containing extra information for the experiments\n",
    "db_info = DB_INFO\n",
    "\n",
    "# split a sample in several mini-samples (chunks) of fixed size\n",
    "# chunksize is optional, but disable it for plotting data\n",
    "#chunk_size = CHUNK_SIZE\n",
    "chunk_size = None\n",
    "\n",
    "# header is optional for general use, mandatory for plotting data\n",
    "#header_names = HEADER\n",
    "header_names = None\n",
    "# overwrite this array with new features if neccesary\n",
    "#header_names = ['f1', 'f2', ...]\n",
    "\n",
    "# load data\n",
    "raw_samples, raw_targets, raw_groups, raw_tasks = load_db(db_dir, db_info, \n",
    "                                                          chunk_size=chunk_size, \n",
    "                                                          header_names=header_names)\n",
    "\n",
    "# create a copy of the original data (for filtering and postprocessing)\n",
    "targets = np.copy(raw_targets) \n",
    "groups = np.copy(raw_groups)\n",
    "tasks = np.copy(raw_tasks)\n",
    "\n",
    "# create an numpy array (copy)\n",
    "samples = np.array(map(np.array, raw_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_test(test):\n",
    "    on_surface = test[test['on_surface'] == 1]\n",
    "    on_air = test[test['on_surface'] != 1]\n",
    "\n",
    "    plt.plot(on_surface['x'], on_surface['y'], '.r')\n",
    "    plt.plot(on_air['x'], on_air['y'], '.b')    \n",
    "\n",
    "def test_selector(subject_id, task_id):\n",
    "    # use global variables (they shouldn't be modified)\n",
    "    global raw_groups, raw_samples, raw_tasks\n",
    "    index = np.logical_and(raw_groups == subject_id, raw_tasks == task_id)\n",
    "    index = np.where(index)[0][0]\n",
    "    return raw_samples[index]\n",
    "\n",
    "#plot_test(test_selector(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop list of tasks\n",
    "# default: just task 1 (spiral)\n",
    "drop_tasks = [1]\n",
    "for task_id in drop_tasks:\n",
    "    keep_tasks = tasks != task_id\n",
    "    samples, targets, groups, tasks = samples[keep_tasks], targets[keep_tasks], groups[keep_tasks], tasks[keep_tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a long vector to compute metrics among all samples\n",
    "joined_samples = np.concatenate(samples, axis=0)\n",
    "\n",
    "# compute mean and std by feature\n",
    "mean_by_feature = np.mean(joined_samples, axis=0)\n",
    "std_by_feature = np.std(joined_samples, axis=0)\n",
    "standarize = lambda a :(a-mean_by_feature) / std_by_feature\n",
    "\n",
    "# compute min and max by feature\n",
    "min_by_feature = np.min(joined_samples, axis=0)\n",
    "max_by_feature = np.max(joined_samples, axis=0)\n",
    "normalize = lambda a :(a-min_by_feature) / np.array(max_by_feature-min_by_feature, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standarize or normalize the samples\n",
    "is_standarization = True\n",
    "\n",
    "if is_standarization:\n",
    "    samples = np.array(map(standarize, samples))\n",
    "else:\n",
    "    samples = np.array(map(normalize, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time x features)\n",
      "Samples shape: (524L, 7991L, 16L)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# max number of timesteps\n",
    "max_timesteps = np.max(map(len, samples))\n",
    "\n",
    "# pad sequence with zero\n",
    "print('Pad sequences (samples x time x features)')\n",
    "padded_samples = sequence.pad_sequences(samples, maxlen=max_timesteps, padding='post', dtype=np.float32)\n",
    "print('Samples shape:',  padded_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (524L, 7991L, 14L)\n"
     ]
    }
   ],
   "source": [
    "# drop selected features \n",
    "# default: drop time\n",
    "#drop_features = [1]\n",
    "# drop more features\n",
    "drop_features = [1, 2]\n",
    "\n",
    "# list of remaining features\n",
    "keep_features = np.delete(np.arange(padded_samples.shape[2]), drop_features)\n",
    "final_samples = np.delete(padded_samples, drop_features, axis=2)\n",
    "n_features = final_samples.shape[2]\n",
    "\n",
    "print('Final shape:',  final_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class StratifiedGroupShuffleSplit():\n",
    "    def __init__(self, test_size=0.2, random_state=None):\n",
    "        self._random_state = check_random_state(random_state)\n",
    "        self._test_size = test_size\n",
    "        self._n_splits=int(1./test_size)\n",
    "        \n",
    "    def get_n_splits(self, X, y, g):\n",
    "        \"\"\"\n",
    "        Returns the number of splitting iterations in the cross-validator. Input parameters \n",
    "        always ignored, exists for compatibility.\n",
    "        \"\"\"\n",
    "        return self._n_splits\n",
    "    \n",
    "    def split(self, X, y, g):\n",
    "        y_pos = y == 1\n",
    "        y_pos_idx = np.where(y_pos)[0]\n",
    "        \n",
    "        X_p, y_p, g_p = X[y_pos], y[y_pos], g[y_pos]\n",
    "        group_splitter_p = GroupShuffleSplit(n_splits=self._n_splits, test_size=self._test_size, \n",
    "                                             random_state=self._random_state).split(X_p, y_p, g_p)\n",
    "        y_neg = y == 0\n",
    "        y_neg_idx = np.where(y_neg)[0]\n",
    "        \n",
    "        X_n, y_n, g_n = X[y_neg], y[y_neg], g[y_neg]\n",
    "        group_splitter_n = GroupShuffleSplit(n_splits=self._n_splits, test_size=self._test_size, \n",
    "                                             random_state=self._random_state).split(X_n, y_n, g_n)\n",
    "\n",
    "        for k in range(self._n_splits):\n",
    "            train_index_p, test_index_p = next(group_splitter_p)\n",
    "            train_index_n, test_index_n = next(group_splitter_n)\n",
    "\n",
    "            train_index = np.hstack([y_pos_idx[train_index_p], y_neg_idx[train_index_n]])        \n",
    "            test_index  = np.hstack([y_pos_idx[test_index_p], y_neg_idx[test_index_n]])\n",
    "\n",
    "            self._random_state.shuffle(train_index)            \n",
    "            self._random_state.shuffle(test_index)\n",
    "            \n",
    "            yield (train_index, test_index)\n",
    "\n",
    "def stratified_group_train_test_split(X, y, g, test_size=0.2, random_state=None, output_groups=False):\n",
    "    splitter = StratifiedGroupShuffleSplit(test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    for train, test in splitter.split(X, y, g):\n",
    "        # exit after 1 iteration\n",
    "\n",
    "        if output_groups:\n",
    "            return X[train], X[test], y[train], y[test], g[train], g[test]\n",
    "        else:\n",
    "            return X[train], X[test], y[train], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random number generator (for experiments reproduction)\n",
    "seed = 42\n",
    "rs = check_random_state(seed)\n",
    "\n",
    "# Split dataset in train and test partitions\n",
    "test_size = 1./5\n",
    "X_train, X_test, y_train, y_test, g_train, g_test = stratified_group_train_test_split(final_samples, targets, groups, test_size, random_state=rs, output_groups=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset for k-fold validation\n",
    "val_test_size=1./5\n",
    "k_fold_splitter = StratifiedGroupShuffleSplit(val_test_size, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Masking\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def create_model(mtype='lstm', max_timesteps=None, n_features=None, units=None, dropout_rate=0.0, learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    # skip zero padding\n",
    "    model.add(Masking(mask_value=0., input_shape=(max_timesteps, n_features)))\n",
    "\n",
    "    # weights initialization may be important\n",
    "    print('Choosen model: %s' % (mtype,))\n",
    "\n",
    "    weights_init ='glorot_uniform'\n",
    "    internal_activation = 'tanh'\n",
    "    if mtype == 'lstm':\n",
    "        model.add(LSTM(units, dropout_W=0.2, dropout_U=0.2, init=weights_init, activation=internal_activation))\n",
    "    elif mtype == 'simple':\n",
    "        model.add(SimpleRNN(units, dropout_W=0.2, dropout_U=0.2, init=weights_init, activation=internal_activation))\n",
    "    elif mtype == 'gru':\n",
    "        model.add(GRU(units, dropout_W=0.2, dropout_U=0.2, init=weights_init, activation=internal_activation))        \n",
    "    else:\n",
    "        print('Model %s not implementd: %s' % (mtype,))       \n",
    "        return None\n",
    "        \n",
    "    # dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # basic activation functions can be relu, tanh or sigmoid\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    optimizer = Adam(lr=learning_rate, decay=1e-6)    \n",
    "    \n",
    "    # REVIEW: is binary_crossentropy the correct loss function ? \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-02585adb0547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Emanuel\\Anaconda2\\lib\\site-packages\\keras\\utils\\visualize_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     raise RuntimeError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     14\u001b[0m                        ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'units': [16, 32, 64, 128], 'dropout_rate': [0, 0.25, 0.5], 'learning_rate': [0.005], 'n_features': [n_features], 'max_timesteps': [max_timesteps],},\n",
    "]\n",
    "\n",
    "batch_size = 64\n",
    "nb_epoch = 10\n",
    "\n",
    "scikit_model = KerasClassifier(build_fn=create_model, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2)\n",
    "\n",
    "grid = GridSearchCV(estimator=scikit_model, param_grid=param_grid, \n",
    "                    n_jobs=1, iid=False, cv=k_fold_splitter, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train, g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_result_df = pd.DataFrame(grid_result.cv_results_)\n",
    "grid_result_file = os.path.join(BASE_DIR, 'grid_results.csv')\n",
    "grid_result_df.to_csv(grid_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best parameters obtained from k-fold validation\n",
    "lstm_units = 32\n",
    "dropout_rate=0.2\n",
    "learning_rate=0.01\n",
    "\n",
    "print('Build model...')\n",
    "model = create_model(mtype='lstm', max_timesteps=max_timesteps, n_features=n_features, units=lstm_units, dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "def run_action(action, **kwargs):   \n",
    "    if action == 'reset':\n",
    "        # reset internal state if needed\n",
    "        model = kwargs['model']\n",
    "        model.reset_states()\n",
    "        return model\n",
    "    \n",
    "    elif action == 'load':\n",
    "        # load internal state from file\n",
    "        if os.path.exists(kwargs['path']):\n",
    "            return load_model(kwargs['path'])\n",
    "        else:\n",
    "            print('Error: File %s not found.' % (kwargs['path'],))            \n",
    "\n",
    "    elif action == 'save':\n",
    "        # save internal state\n",
    "        model = kwargs['model']\n",
    "        model.save(kwargs['path'])\n",
    "        return model\n",
    "    else:\n",
    "        print('Error: Action %s not implemented' % (action,))\n",
    "\n",
    "# Examples\n",
    "#fname = os.path.join(BASE_DIR, 'lstm_' + str(lstm_units) + '.h5')\n",
    "#model = run_action('load', path=fname)\n",
    "#run_action('save', path=fname, model=model)\n",
    "#run_action('reset', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "from keras.models import load_model\n",
    "\n",
    "batch_size = 64\n",
    "nb_epoch = 20\n",
    "\n",
    "cbks = [ \n",
    "        callbacks.History(),     \n",
    "#       callbacks.ModelCheckpoint(filepath=fname, monitor='val_loss', save_best_only=True),\n",
    "#       callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "#       callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "#       callbacks.LearningRateScheduler(step_decay),\n",
    "       ]\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2, \n",
    "#                    validation_data=(X_test, y_test), \n",
    "                    callbacks=cbks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# create scikit wrapper\n",
    "scikit_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# workaround\n",
    "scikit_model.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_preds = scikit_model.predict(X_train)\n",
    "acc = accuracy_score(y_train, train_preds)\n",
    "print('Train accuracy = {:.4f}'.format(acc))\n",
    "print(classification_report(y_train, train_preds, target_names=['control', 'parkinson']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = scikit_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, test_preds)\n",
    "print('Test accuracy = {:.4f}'.format(acc))\n",
    "print(classification_report(y_test, test_preds, target_names=['control', 'parkinson']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.MIMEMultipart import MIMEMultipart\n",
    "from email.MIMEText import MIMEText\n",
    "from email.MIMEBase import MIMEBase\n",
    "from email import encoders\n",
    " \n",
    "fromaddr = \"juanluisrosaramos@gmail.com\"\n",
    "toaddr = \"juanluisrosaramos@gmail.com\"\n",
    " \n",
    "msg = MIMEMultipart()\n",
    " \n",
    "msg['From'] = fromaddr\n",
    "msg['To'] = toaddr\n",
    "msg['Subject'] = \"CI RESULTS!\"\n",
    " \n",
    "body = \"TEXT YOU WANT TO SEND\"\n",
    " \n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    " \n",
    "filename = os.path.join(BASE_DIR, 'lstm_' + str(32) + '.h5')\n",
    "attachment = open(filename, \"rb\")\n",
    " \n",
    "part = MIMEBase('application', 'octet-stream')\n",
    "part.set_payload((attachment).read())\n",
    "encoders.encode_base64(part)\n",
    "part.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    " \n",
    "msg.attach(part)\n",
    " \n",
    "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "server.starttls()\n",
    "server.login(fromaddr, \"dsrtrtyu\")\n",
    "text = msg.as_string()\n",
    "server.sendmail(fromaddr, toaddr, text)\n",
    "server.quit()\n",
    "attachment.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
